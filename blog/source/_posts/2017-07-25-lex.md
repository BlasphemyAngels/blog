---
title: Lex模块的简单使用
date: 2017-07-25 14:27:02
categories:
  - python
tags:
  - lex
  - 正则表达式
---

## 正文

&ensp;&ensp;&ensp;[PLY](http://www.dabeaz.com/ply/ply.html)是由python实现的编译器构建工具，其中包含`lex`和`yacc`两个模块。`lex.py`模块用来将输入字符通过一系列的正则表达式分解成标记序列，`yacc.py`通过一些上下文无关的文法来识别编程语言语法。`yacc.py`使用`LR`解析法，并使用`LALR(1)`算法（默认）或者`SLR`算法生成分析表。

&ensp;&ensp;&ensp;本文主要介绍`lex.py`，因为我最近用到它做一些字符串方面的数据处理问题，跟编译没关系，也就没看`yacc`。

<!--more-->

### 安装

&ensp;&ensp;&ensp;安装很简单：

* pip 安装：`pip install ply`
* 源码：[PLY main](http://www.dabeaz.com/ply/)下载完后使用`tar -zxvf`解压后可以复制里面的`lex.py`出来引用就可以了。

### 简介

&ensp;&ensp;&ensp;`lex.py`将输入的串解析为一系列自定义的字符串。如输入串是下面：

`for i range(1, 10)`

&ensp;&ensp;&ensp;`lex`将串分解为标记：

`for`, `i`, `range`, `(`, `1`, `,`, `10`, `)`

&ensp;&ensp;&ensp;为了更好的区分和使用标记，每类标记取一个类型名字(type):

如：

&ensp;&ensp;&ensp;`ID`, `NUMBER`, `LPAREN`, `RPAREN`, 等

那么原始串就是：

(`'FOR'`, `'for'`), (`'ID'`, `'x'`), (`'IN'`, `'in'`), (`'RANGE'`, `'range'`), (`'LPAREN'`, `'('`), (`'NUMBER'`, '1'), ...


### 用法

```python
import ply.lex as lex

# List of token names.   This is always required
tokens = (
   'NUMBER',
   'PLUS',
   'MINUS',
   'TIMES',
   'DIVIDE',
   'LPAREN',
   'RPAREN',
)

# Regular expression rules for simple tokens
t_PLUS    = r'\+'
t_MINUS   = r'-'
t_TIMES   = r'\*'
t_DIVIDE  = r'/'
t_LPAREN  = r'\('
t_RPAREN  = r'\)'

# A regular expression rule with some action code
def t_NUMBER(t):
    r'\d+'
    t.value = int(t.value)
    return t

# Define a rule so we can track line numbers
def t_newline(t):
    r'\n+'
    t.lexer.lineno += len(t.value)

# A string containing ignored characters (spaces and tabs)
t_ignore  = ' \t'

# Error handling rule
def t_error(t):
    print "Illegal character '%s'" % t.value[0]
    t.lexer.skip(1)

# Build the lexer
lexer = lex.lex()
```



&ensp;&ensp;&ensp;上面的方式定义好了`lex`，下面的代码使用它:

```python
# Test it out
data = '... '

# Give the lexer some input
lexer.input(data)

# Tokenize
while True:
    tok = lexer.token()
    if not tok: break      # No more input
    print tok
for tok in lexer:
    print tok
    ```

&ensp;&ensp;&ensp;上面给出了两种迭代方式。

&ensp;&ensp;&ensp;由`lexer.token()`方法返回的标记是LexToken类型的实例，拥有`tok.type`,`tok.value`,`tok.lineno`和`tok.lexpos`属性，其中`tok.type`和`tok.value`分别代表标记的类型和值一个`t.type`的属性（字符串表示）来表示标记的类型名称，`t.value`是标记值（匹配的实际的字符串），`t.lineno`表示当前在源输入串中的作业行，`t.lexpos`表示标记相对于输入串起始位置的偏移

#### 标记列表

&ensp;&ensp;&ensp;上述代码中声明了一个标记列表，标记列表可以人为给每种类型标记起个名字，然后在后面方便使用。

```python
tokens = (
   'NUMBER',
   'PLUS',
   'MINUS',
   'TIMES',
   'DIVIDE',
   'LPAREN',
   'RPAREN',
)
```
#### 标记的规则

&ensp;&ensp;&ensp;每种标记用一个正则表达式规则来表示，每个规则需要以`t_`开头声明，表示该声明是对标记的规则定义。对于简单的标记，可以定义成这样（在Python中使用raw string能比较方便的书写正则表达式）：
`t_PLUS = r'\+'`

&ensp;&ensp;&ensp;这里，紧跟在`t_`后面的单词，必须跟标记列表中的某个标记名称对应。如果需要执行动作的话，规则可以写成一个方法。例如，下面的规则匹配数字字串，并且将匹配的字符串转化成`Python`的整型：

```python
def t_NUMBER(t):
    r'\d+'
    t.value = int(t.value)
    return t
    ```

&ensp;&ensp;&ensp;如果使用方法的话，正则表达式写成方法的文档字符串。方法总是需要接受一个`LexToken`实例的参数，该实例有一些属性（上面已经介绍）。方法可以在方法体里面修改这些属性。记住如果再方法中改变了属性，那么要返回结果token（如上面代码中的`return  t`)，否则，标记将被丢弃。

&ensp;&ensp;&ensp;在`lex`内部，`lex`.py用re模块处理模式匹配，在构造最终的完整的正则式的时候，用户提供的规则按照下面的顺序加入：

* 所有由方法定义的标记规则，按照他们的出现顺序依次加入
* 由字符串变量定义的标记规则按照其正则式长度倒序后，依次加入（长的先入）

&ensp;&ensp;&ensp;顺序的约定对于精确匹配是必要的。比如，如果你想区分`=`和`==`，你需要确保`==`优先检查。如果用字符串来定义这样的表达式的话，通过将较长的正则式先加入，可以帮助解决这个问题。用方法定义标记，可以显示地控制哪个规则优先检查。

#### 标记的值

&ensp;&ensp;&ensp;标记被`lex`返回后，它们的值被保存在`value`属性中。正常情况下，`value`是匹配的实际文本。事实上，`value`可以被赋为任何`Python`支持的类型。例如，当扫描到标识符的时候，你可能不仅需要返回标识符的名字，还需要返回其在符号表中的位置，可以像下面这样写：

```python

def t_ID(t):
    ...
    # Look up symbol table information and return a tuple
    t.value = (t.value, symbol_lookup(t.value))
    ...
    return t
    ```
#### 丢弃标记

&ensp;&ensp;&ensp;想丢弃像注释之类的标记，只要不返回`value`就行了，像这样：

```python

def t_COMMENT(t):
    r'\#.*'
    pass
    # No return value. Token discarded
    ```
&ensp;&ensp;&ensp;为标记声明添加`ignore_`前缀同样可以达到目的：

`t_ignore_COMMENT = r'\#.*'`

&ensp;&ensp;&ensp;如果有多种文本需要丢弃，建议使用方法来定义规则，因为方法能够提供更精确的匹配优先级控制（方法根据出现的顺序，而字符串的正则表达式依据正则表达式的长度）

#### 行号和位置信息

&ensp;&ensp;&ensp;默认情况下，`lex.py`是不提供行号的。因为`lex.py`根本不知道何为”行”的概念（换行符本身也作为文本的一部分）。不过，可以通过写一个特殊的规则来记录行号：

```python
def t_newline(t):
    r'\n+'
    t.lexer.lineno += len(t.value)
    ```
&ensp;&ensp;&ensp;在这个规则中，当前`lexer`对象`t.lexer`的`lineno`属性被修改了，而且空行被简单的丢弃了，因为没有任何的返回。

&ensp;&ensp;&ensp;`lex.py`也不自动做列跟踪。但是，位置信息被记录在了每个标记对象的`lexpos`属性中，这样，就有可能来计算列信息了。例如：每当遇到新行的时候就重置列值：

```python
# Compute column.
#     input is the input text string
#     token is a token instance
def find_column(input,token):
    last_cr = input.rfind('\n',0,token.lexpos)
    if last_cr < 0:
        last_cr = 0
    column = (token.lexpos - last_cr) + 1
    return column

```

&ensp;&ensp;&ensp;通常，计算列的信息是为了指示上下文的错误位置，所以只在必要时有用。

#### 忽略字符

&ensp;&ensp;&ensp;`t_ignore`规则比较特殊，是`lex.py`所保留用来忽略字符的，通常用来跳过空白或者不需要的字符。虽然可以通过定义像`t_newline()`这样的规则来完成相同的事情，不过使用`t_ignore`能够提供较好的词法分析性能，因为相比普通的正则式，它被特殊化处理了。


#### 字面字符

&ensp;&ensp;&ensp;字面字符可以通过在词法模块中定义一个literals变量，例如：

`literals = [ '+','-','*','/' ]`

或者：

`literals = "+-*/"`

&ensp;&ensp;&ensp;字面字符是指单个字符，表示把字符本身作为标记，标记的type和value都是字符本身。不过，字面字符是在其他正则式之后被检查的，因此如果有规则是以这些字符开头的，那么这些规则的优先级较高。

#### 错误处理

&ensp;&ensp;&ensp;最后，在词法分析中遇到非法字符时，`t_error()`用来处理这类错误。这种情况下，`t.value`包含了余下还未被处理的输入字串，在之前的例子中，错误处理方法是这样的：

```python
# Error handling rule
def t_error(t):
    print "Illegal character '%s'" % t.value[0]
    t.lexer.skip(1)
    ```
&ensp;&ensp;&ensp;这个例子中，我们只是简单的输出不合法的字符，并且通过调用`t.lexer.skip(1)`跳过一个字符。

#### 构建使用lexer

&ensp;&ensp;&ensp;函数`lex.lex()`使用Python的反射机制读取调用上下文中的正则表达式，来创建`lexer`。`lexer`一旦创建好，有两个方法可以用来控制`lexer`对象：

* `lexer.input(data)` 重置`lexer`和输入字串
* `lexer.token()` 返回下一个`LexToken`类型的标记实例，如果进行到输入字串的尾部时将返回`None`

&ensp;&ensp;&ensp;推荐直接在`lex()`函数返回的`lexer`对象上调用上述接口，尽管也可以向下面这样用模块级别的`lex.input()`和`lex.token()`：

```python

lex.lex()
lex.input(sometext)
while 1:
    tok = lex.token()
    if not tok: break
    print tok
    ```

&ensp;&ensp;&ensp;在这个例子中，`lex.input()`和`lex.token()`是模块级别的方法，在lex模块中，`input()`和`token()`方法绑定到最新创建的`lexer`对象的对应方法上。最好不要这样用，因为这种接口可能不知道在什么时候就失效。

#### `@TOKEN`装饰

&ensp;&ensp;&ensp;在一些应用中，你可能需要定义一系列辅助的记号来构建复杂的正则表达式，例如：

```python
digit            = r'([0-9])'
nondigit         = r'([_A-Za-z])'
identifier       = r'(' + nondigit + r'(' + digit + r'|' + nondigit + r')*)'

def t_ID(t):
    # want docstring to be identifier above. ?????
    ...
    ```
&ensp;&ensp;&ensp;在这个例子中，我们希望`ID`的规则引用上面的已有的变量。然而，使用文档字符串无法做到，为了解决这个问题，你可以使用`@TOKEN`装饰器：

```python

from ply.lex import TOKEN

@TOKEN(identifier)
def t_ID(t):
    ...

```
&ensp;&ensp;&ensp;装饰器可以将`identifier`关联到`t_ID()`的文档字符串上以使`lex.py`正常工作，一种等价的做法是直接给文档字符串赋值：

```python
def t_ID(t):
    ...

t_ID.__doc__ = identifier
```

#### 调试

&ensp;&ensp;&ensp;如果想要调试，可以使`lex()`运行在调试模式：

`lexer = lex.lex(debug=1)`

&ensp;&ensp;&ensp;这将打出一些调试信息，包括添加的规则、最终的正则表达式和词法分析过程中得到的标记。

&ensp;&ensp;&ensp;除此之外，`lex.py`有一个简单的主函数，不但支持对命令行参数输入的字串进行扫描，还支持命令行参数指定的文件名：

```python
if __name__ == '__main__':
     lex.runmain()
     ```
&ensp;&ensp;&ensp;想要了解高级调试的详情，请移步至最后的高级调试部分。

####  其他方式定义词法规则

&ensp;&ensp;&ensp;上面的例子，词法分析器都是在单个的Python模块中指定的。如果你想将标记的规则放到不同的模块，使用`module`关键字参数。例如，你可能有一个专有的模块，包含了标记的规则：

```python

# module: tokrules.py
# This module just contains the lexing rules

# List of token names.   This is always required
tokens = (
   'NUMBER',
   'PLUS',
   'MINUS',
   'TIMES',
   'DIVIDE',
   'LPAREN',
   'RPAREN',
)

# Regular expression rules for simple tokens
t_PLUS    = r'\+'
t_MINUS   = r'-'
t_TIMES   = r'\*'
t_DIVIDE  = r'/'
t_LPAREN  = r'\('
t_RPAREN  = r'\)'

# A regular expression rule with some action code
def t_NUMBER(t):
    r'\d+'
    t.value = int(t.value)
    return t

# Define a rule so we can track line numbers
def t_newline(t):
    r'\n+'
    t.lexer.lineno += len(t.value)

# A string containing ignored characters (spaces and tabs)
t_ignore  = ' \t'

# Error handling rule
def t_error(t):
    print "Illegal character '%s'" % t.value[0]
    t.lexer.skip(1)
    ```
&ensp;&ensp;&ensp;现在，如果你想要从不同的模块中构建分析器，应该这样（在交互模式下）：

```python

>>> import tokrules
>>> lexer = lex.lex(module=tokrules)
>>> lexer.input("3 + 4")
>>> lexer.token()
LexToken(NUMBER,3,1,1,0)
>>> lexer.token()
LexToken(PLUS,'+',1,2)
>>> lexer.token()
LexToken(NUMBER,4,1,4)
>>> lexer.token()
None
```

&ensp;&ensp;&ensp;`module`选项也可以指定类型的实例，例如：

```python

import ply.lex as lex

class MyLexer:
    # List of token names.   This is always required
    tokens = (
       'NUMBER',
       'PLUS',
       'MINUS',
       'TIMES',
       'DIVIDE',
       'LPAREN',
       'RPAREN',
    )

    # Regular expression rules for simple tokens
    t_PLUS    = r'\+'
    t_MINUS   = r'-'
    t_TIMES   = r'\*'
    t_DIVIDE  = r'/'
    t_LPAREN  = r'\('
    t_RPAREN  = r'\)'

    # A regular expression rule with some action code
    # Note addition of self parameter since we're in a class
    def t_NUMBER(self,t):
        r'\d+'
        t.value = int(t.value)
        return t

    # Define a rule so we can track line numbers
    def t_newline(self,t):
        r'\n+'
        t.lexer.lineno += len(t.value)

    # A string containing ignored characters (spaces and tabs)
    t_ignore  = ' \t'

    # Error handling rule
    def t_error(self,t):
        print "Illegal character '%s'" % t.value[0]
        t.lexer.skip(1)

    # Build the lexer
    def build(self,**kwargs):
        self.lexer = lex.lex(module=self, **kwargs)

    # Test it output
    def test(self,data):
        self.lexer.input(data)
        while True:
             tok = lexer.token()
             if not tok: break
             print tok

# Build the lexer and try it out
m = MyLexer()
m.build()           # Build the lexer
m.test("3 + 4")     # Test it

```

#### 额外状态维护

&ensp;&ensp;&ensp;在你的词法分析器中，你可能想要维护一些状态。这可能包括模式设置，符号表和其他细节。例如，假设你想要跟踪NUMBER标记的出现个数。

&ensp;&ensp;&ensp;一种方法是维护一个全局变量：

```python

num_count = 0
def t_NUMBER(t):
    r'\d+'
    global num_count
    num_count += 1
    t.value = int(t.value)
    return t
    ```

&ensp;&ensp;&ensp;除此之外，还可以还可以记录信息到lexer对象内部。可以通过当前标记的lexer属性访问：

```python
def t_NUMBER(t):
    r'\d+'
    t.lexer.num_count += 1     # Note use of lexer attribute
    t.value = int(t.value)    
    return t

lexer = lex.lex()
lexer.num_count = 0            # Set the initial count
```

&ensp;&ensp;&ensp;还可以自定义你的lexer类型：

```python
class MyLexer:
    ...
    def t_NUMBER(self,t):
        r'\d+'
        self.num_count += 1
        t.value = int(t.value)
        return t

    def build(self, **kwargs):
        self.lexer = lex.lex(object=self,**kwargs)

    def __init__(self):
        self.num_count = 0
        ```

## 参考链接
* [PLY (Python Lex-Yacc)](http://www.dabeaz.com/ply/ply.html)
