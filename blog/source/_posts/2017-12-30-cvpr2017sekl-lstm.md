---
title: Skeleton Key-Image Captioning by Skeleton-Attribute Decomposition
copyright: true
date: 2018-01-05 15:26:21
categories:
  - papper
tags:
  - papper
  - ImageCaption
  - CVPR
mathjax: true
---

<blockquote>
最近在读`CVPR2017`的关于图像描述的文章，读到本文感到文章结构简单但是设置巧妙，所以特地markdown一下
</blockquote>

[论文地址](https://arxiv.org/abs/1704.06972)

<!--more-->

#### 摘要

最近，图像描述任务得到广泛关注。大多数已经存在的基于语言模型的方法是依据原先描述的词的顺序一个词一个词的生成。然而对于人，更自然的方法是，先定位好物体以及它们之间的关系，然后仔细描述每个物体以及它们的属性。文章提出一种“从粗糙到细致”的图像描述方法，通过将原始图像分解为一个主干句子和一些属性，这两者的生成是分开的。通过这种分解，文章的方法可以使得描述更加准确，生成更加合理的描述。而且还能生成长度不一更自然的描述。

#### 引言

近几年，自动生成图像的描述问题在计算机视觉领域受到了广泛的关注。但是这个问题是很有挑战性的，原因是描述的生成过程不仅需要学习到高层的图像语义而不仅仅是简单的物体和场景的识别，再就是需要产生一个语义和语法都正确的句子来描述重要的物体、物体间的属性以及它们之间的关系。

图像描述模型的发展历程可以总结为下图:
![image_caption_c](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/image_caption_c.png?raw=true
)

从图中可以看出，图像描述模型分为三个流派：
* 基于检索
* 基于模板
* 基于语言模型

基于检索的方法是通过对输入的图像在一个庞大的图像-文本对数据集中进行检索，得到相应的描述。这种方法的结果好坏依赖数据集的好坏和完整度，数据集很差或不完整都会影响描述结果。

基于模板的方法是根据预先设定好的模板进行生成描述，如预先设定好的模板可能是`主谓宾`，这样生成的描述依赖模板的好坏，而且模板是有限的，而句子是无限的，以有限描述无限会使生成的描述显得死板，不自然。

基于语言模型的方式是通过神经网络计算给定图像下得到描述的似然概率生成描述，训练的过程就是最大化这个似然概率。但是基于语言的方法会有下面两方面的问题：

* 泛化能力不够，生成的描述有可能只是对训练数据的"死记硬背"
* 一个词一个词的生成描述，有可能使得生成的物体在其属性的后面生成。

为了解决这些问题，文章提出了一种从粗糙到细致的描述生成方法，方法分两步：
* 第一步先生成描述的基本主干，基本主干中包含了图像中主要物体。
* 第二步对基本主干使用注意力生成相应的属性。

这样将描述物体和描述属性两个过程分离开，使得描述生成的更准确。

如下图：
![skeleton](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/skeleton.png?raw=true
)

本文的工作的灵感来自于认知神经学研究。文章介绍在视觉感知的过程如物体识别的过程中，有一种从上到下进行识别的机制起了重大作用，这种机制的原理还没有被人们索理解，但是有科学家猜测其先是通过低频率的空间特征得到对一个物体的初始猜测。然后低频率空间特征被映射为低层次视觉特征，用于得到高层次的语义。这个思想借鉴到图像描述任务就是从粗糙到精细的生成描述思想，通过将问题分解为两步，先描述句子概况，然后通过句子概况描述相应细节，前一步有助于解决下一步。

#### 模型

模型总体结构如下图，在训练阶段，将描述分解为主干句子和属性输入到模型中训练，测试阶段先基于图像生成主干，然后在主干句子上施加注意力产生相应的属性，然后将主干句子和属性合并成描述。使用两个lstm，一个是`skel-lstm`,用于对输入的图像产生骨架，另一个是`attr-lstm`用来得到主干句子中物体相应的属性。

![sekelon2](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/sekelon2.png?raw=true
)

##### 描述的骨架－属性分解

文章使用斯坦福的`NLP`工具`constituency parser`对句子进行分析，得到一个依赖树，如上图中。树中的结点对应短语的不同类型，如`NP`(名词短语)，`VP`(动词短语)，`PP`(介词短语)和`AP`(形容词短语)等。

那么怎么抽取句子中的物体呢？找到最低层的`NP`，将`NP`中的最后一个词视为物体，最后一个之前的词视为这个物体的属性。而其他属性的结点则保持不变的留在主干句子中。

有时上面的方式提取物体看起来不太适合，如对于`coffee cup`是将`cup`看做物体，将`coffee`看做描述`cup`的属性还是将`coffee cup`看做一个物体好?本文不区分这些，都使用上面的方式。实验效果显示模型会学到到它们内在的关系。

##### coarse-of-fine LSTM

模型编码阶段使用`CNN`对图像进行编码，解码阶段使用两个`LSTM`，一个`skel-lstm`生成句子主干，一个`attr-lstm`生成为骨架中物体生成相应的属性。

###### skel-lstm

`skel-lstm`使用的`soft attention lstm`，其结构如下图所示：

![soft-atten](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/lstm-att.png?raw=true)

可以看出，上图其实就是一个基本的`LSTM`结构，只是它的每个门的输入多了注意力向量\\(z\_t\\)和前一个时刻`LSTM`的输出`$Ey_{t-1}$`。

其中的`$z_t$`就是注意力映射向量，这个向量的计算方式如下：

我们使用`$v_{ij}\in R^D$`表示在位置`$(i,j)\in L\times L$`处的图像特征。第`t`步的注意力映射表示为归一化的权值`$\alpha_{ij,t}$`，它是通过下面的式子得到:
```math
\alpha{ij,t}=Softmax(MLP(v_{i,j}, h_{t-1}))
```
然后就能得到`$z_t$`：
```math
z_t=\sum_{i,j}\alpha_{ij,t}v_{ij}
```

我们计算得到的注意力映射向量`$z_t$`不仅用于此处生成句子主干，而且还重用于生成后面的物体属性。

##### Attr-Lstm

在句子主干生成后，我们需要为句子主干中的物体生成相应的属性，文章使用`Attr-lstm`直接生成一系列的属性，而不是分别为一个物体生成多个属性。

`Attr-lstm`的结构与传统用于图像描述的`lstm`结构类似，如下图：
![lstm-caption](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/LSTM-caption.png?raw=true
)

上图中的LSTM的输入序列是：
```math
x_{-1}=CNN(I) 

x_t=W_ey_t,t=0,1,...,N-1
```
其中`I`代表图像，`CNN(I)`代表图像的特征，`$W_e$`是词嵌入矩阵，`$y_t$`是词的`one-hot`向量，所以`$W_ey_t$`是单词的嵌入向量。

作者认为属性的生成与原图像、主干单词、主干句子的上下文这三种类型的信息相关。而主干句子的上下文信息就存储在`skel-lstm`中，那么在`attr-lstm`的第`t`不使用哪个`skel-lstm`哪一步的状态信息比较好呢？作者通过实验使用`skel-lstm`的前一个状态、当前状态，下一个状态的信息，结果发现当前步状态信息效果最好。而图像特征也不再使用原始图像特征，而是经过`skel-lstm`计算后的注意力映射特征`$z_t$`。

综上，在`Attr-lstm`中`lstm`的输入是:
```math
x_{-1}=MLP(W_1z_T, W_tS_T^{skel}+W_hh_T^{sekl})

x_t=W_ey_t,t=0,1,...,N-1
```
其中，`$z_t$`就是上面提到的经过注意力映射的图特征，`$S_T^{skel}$`是第`T`个主干句子中的词向量，`$h_T^{sekl}$`是第`T`步`skel-lstm`的隐状态。

##### 精调`$z_t$`

考虑到注意力在序列到序列模型中的重要性，作者提出了一种对上面的注意力映射向量`$\alpha_{ij}$`的改进方式：

上面提到的`$\alpha$`其实是通过前一步的状态得到，所以它是前一单词的注意力，而作者认为，当前词的确定不只依赖前一个词，也依赖后一个词，所以定义了`$\alpha_{post}$`代表后一个词的注意力。

设`$P_{attend}=p_1,P-2,...,p_Q$`为`LSTM`的输出，其中`Q`代表`sekl-lstm`的输入词典大小。那么将图像`I`每个位置特征`$v_{i,j}$`输入到`LSTM`中就能得到一个概率向量`$P_{ij}$`。于是作者就使用这个概率对`$\alpha_{ij}$`进行改进。
```math
\alpha_{post(ij)}=\frac{1}{Z}P^T_{attend}\times P_{ij}
```
其中`Z`是正则化项。

精调其实就是使用`$P_{attend}`和`$P_{ij}$`的相似度作为下一个词的注意力映射向量。

如下图：

![attention-refine](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/attention-refine.png?raw=true
)

##### 长度因子

太短的描述很可能会丢失物体的属性，所以作者在生成描述的时候加入一个长度因子来鼓励长描述的生成。不同长度的描述蕴含了不同丰富度的信息，这允许我们根据用户的偏好来调节产生匹配用户的偏好。

```math
log(\hat{P})=log(P)+\gamma\times l
```
其中`P`是生成的描述的概率。

#### 实验

##### 数据集

实验使用的数据集有两个：

* MS-COCO
* Stock3M

##### 数据预处理

对于`MSCOCO`数据集，移除其中的标点，将描述全部转换为小写。去除在主干句子中出现次数低于5次的单词和在属性句子中出现3次的单词。

##### 实验结果

在`SPICE`上的对比图：

![sekl-expr1](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/skel-expr1.png?raw=true
)

在其他评价标准上的对比图：

![skel-expr2](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/skel-expr1.png?raw=true
)

<hr/>
