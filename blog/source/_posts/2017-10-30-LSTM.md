---
title: 理解LSTM（译）
copyright: true
date: 2017-10-29 19:28:01
password:
mathjax: true
categories:
  - DL
tags:
  - DL
  - LSTM
---

<blockquote>LSTM(long short term memory, 长短记忆网络)广泛应用于文本，很多基于其的模型最近几年被提出来解决时序模型，因此对其的理解有助于将其应用于新问题。本文的工作主要是对[博文](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)进行翻译，以理解LSTM</blockquote>

<!--more-->

### 循环神经网络

&ensp;&ensp;&ensp;人们的思考是持续性的，不是每秒都从头开始重新思考。当你读到这篇文章时，你都是在理解前面已经读过的单词的基础上理解每一个单词，你不会将前面的东西扔了重新思考。因此你的思考具有持续性。

&ensp;&ensp;&ensp;但是传统的神经网络不能做到这样，想一下，如果你想预测电影中每一帧要发生什么，传统神经网络并不能使用电影前面的信息来推断下一帧要发生的事情。

&ensp;&ensp;&ensp;循环神经网络可以做到这一点，通过循环的输入网络，使得信息能够保持住。

![LSTM_RNN](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/LSTM_RNN.png?raw=true)
<center font="8px">循环神经网络</center>

&ensp;&ensp;&ensp;在上图中的网络A，通过将输入\\(x\_t\\)处理输出\\(h\_t\\)。循环的存在使得信息能够从网络的当前步传递到下一步。

&ensp;&ensp;&ensp;上图中循环的存在使得循环神经网络看起来不是那么容易理解。换了角度看，通过将循环神经网络每一步展开，如下图，循环神经网络可以被看做是多个相同网络的复制，每一个网络将信息传递给下一个网络。

![LSTM_RNN_unroll](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/LSTM_RNN_unroll.png?raw=true)
<center font="8px">展开的循环神经网络</center>

&ensp;&ensp;&ensp;这种像链式的结果很自然的使得RNN能够很好的应用与序列数据。

&ensp;&ensp;&ensp;最近几年，循环神经网络被应用到了很多领域：机器翻译、语言模型、语言识别、图像描述等，对这些应用起着重大贡献是LSTMs，一种特殊的RNN，其效果比标准版本的RNN好很多，很多基于循环神经网络的应用使用的都是LSTM，本文讨论的就是它。

### 长短时依赖的问题

&ensp;&ensp;&ensp;RNN的作用是能够将前面的信息跟现在的工作连接起来，例如利用电影中前面的情节预测当前帧要发生什么。如果RNN能做到这一点的话，那么它是很有用的，但是它真的能完全做到这一点吗？不一定。

&ensp;&ensp;&ensp;有时我们只需要最近的信息来预测当前的状态。例如，考虑一个语言模型，通过前面的词预测下一个词是什么，如果我们要预测`"the cloud are in the sky"`中的最后一个词`sky`，不需要太前面的信息，只要前面的`the cloud are in`的信息就可以预测`sky`。在这种情况下，预测所需要的与预测有关的前面的相关信息跟当前位置间隔很小，RNN的表现是很好的。

![LSTM_RNN_gap_s](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/LSTM_RNN_gap_s.png?raw=true)
<center font="8px">相关信息间隔很小</center>

&ensp;&ensp;&ensp;但是有的时候我们在预测时确需要更多的上下文，考虑我们要预测文本`"I grew up in France...I speak fluent Franch"`中的最后一个词。最近的信息只能告诉我们最后一个词是一种语言，而我们要更准确的知道是哪个词，我们需要得到前面的上下文`France`的信息，而`France`距离最后面是很远的。这种情况下，预测所需要的相关信息的间隔很大，很不幸的是，当间隔很大时，RNN不能学习到很远的信息。也就是说在这种情况下，RNN的效果不好。

![LSTM](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/LSTM_RNN_gap_b.png?raw=true)
<center font="8px">相关信息间隔很大</center>

&ensp;&ensp;&ensp;在理论上，RNNs完全能够解决这种`长时间依赖`问题。人们可以很小心的调参使得一些小的RNN-base模型能够有好的效果。但是，在实践中RNN并不能学习到`长时间依赖`信息。原因有很多，其中很重要的是`梯度爆炸`和`梯度消失问题`，详细可以看[(1991) [German]](http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf)和[Bengio, et al. (1994)](http://www-dsi.ing.unifi.it/~paolo/ps/tnn-94-gradient.pdf)。

### LSTM网络

&ensp;&ensp;&ensp;LSTM(Long Short Term Memory Networks,长短时记忆网络)，是一种特殊的RNN网络，它能够学习到`长时间依赖`。它最早被介绍在[Hochreiter & Schmidhuber (1997)](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf)，最近LSTM被人们广泛应用和改进。

&ensp;&ensp;&ensp;所有的RNN网络的网络结构都是以重复单元构成的链式架构。在标准RNN中，这个重复单元是一种非常简单的结构，例如只含有一个单一的`tanh`层。

![LSTM_RNN_repeat_module](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/LSTM_RNN_repeat_module.png?raw=true)
<center font="8px">标准RNN的重复单元，只含有一层tanh</center>

&ensp;&ensp;&ensp;LSTM的结构和RNN一样也是链式的。但是它的重复单元不同。相对于RNN的重复单元只含有一层，LSTM的重复单元是通过特殊方式连接的四层。

![LSTM_LSTM_repeat_module](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/LSTM_LSTM_repeat_moudle.png?raw=true)
<center>LSTM的重复单元：含有相互连接的四层</center>

&ensp;&ensp;&ensp;接下来我们可以一步步的学习LSTM的详细结构。
&ensp;&ensp;&ensp;先来规定一下操作的定义：

![LSTM_LSTM_sy](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/LSTM_LSTM_sy.png?raw=true)
<center>神经网络单元&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;元素级操作&ensp;&ensp;&ensp;&ensp;矩阵转换&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;连接&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;复制&ensp;&ensp;&ensp;</center>

&ensp;&ensp;&ensp;前面的LSTM重复单元的途中，每条线都携带一个向量，从输出结点出来输入到其他结点去。粉红色的圆代表元素级的运算，如矩阵相加等。黄色的方块代表可训练的神经网络层，线合并的地方代表连接，线分开的地方，代表将一个输入复制成两个输出，输出到不同的地方去。

### LSTM的核心概念

&ensp;&ensp;&ensp;LSTM的关键概念是细胞状态，在下图中最上面的水平线中携带的信息就是细胞状态。

&ensp;&ensp;&ensp;细胞状态就像一个传输背包，它在经过整条链的过程中只进行很小的线性变换。所以细胞状态中的信息在通过整个链的过程中变化是很少的。

![LSTM_LSTM_cell_state](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/LSTM_LSTM_cell_state.png?raw=true)
&ensp;&ensp;&ensp;通过呼叫门，LSTM可以向细胞状态中添加或者移除信息。

&ensp;&ensp;&ensp;门的作用是控制信息是否穿过，门由一个`sigmoid`层 和一个`元素级乘法操作`构成。

![LSTM_LSTM_gates](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/LSTM_LSTM_gates.png?raw=true)
&ensp;&ensp;&ensp;`sigmoid`层输出的值是`0~1`，决定组件的多少信息能够通过，如果是`0`代表任何信息都不能通过，如果是`1`代表任何信息都能通过。

&ensp;&ensp;&ensp;LSTM有3个门，来控制和保护细胞状态。

### 一步一步走进LSTM

&ensp;&ensp;&ensp;LSTM的第一步是决定细胞状态的信息哪些需要扔掉，哪些需要保留。这一决策是通过`忘记门`层来完成。`忘记门`通过结合\\(h\_{t-1}和x\_t\\)的信息，为每个\\(C\_{t-1}\\)矩阵的元素生成一个在`0~1`之间的数，这个数的作用就是上面说的门上的`sigmoid`输出的数的作用。

&ensp;&ensp;&ensp;回到前面讨论过的语言模型，在前面所有的词的基础上预测下一个词。在这个问题中，细胞状态的信息肯定包含了前一个主语的信息，而我们如果进入下一个主语预测，那么我们肯定希望忘记前一个主语的信息，也就是将前一个主语的信息从细胞状态中移除。

![LSTM_step1](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/LSTM_step1.png?raw=true)

&ensp;&ensp;&ensp;下一步是决定哪些新信息需要被添加到细胞状态中。分为两步，首先使用一个称为`输入门`的`sigmoid`层决定哪些信息能够被加入。然后使用一个`tanh`层结合\\(h\_{t-1}和x\_t\\)为一个新的候选值\\(\tilde{C\_t}\\)，最后将这两步的输出值结合起来得到添加到细胞状态的信息。
&ensp;&ensp;&ensp;回到语言模型模型，就是上一步已经将前一个主语丢掉了，这一步就是在细胞状态中加入新主语信息。

![LSTM_step2](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/LSTM_step2.png?raw=true)

&ensp;&ensp;&ensp;现在更新细胞状态，将其从\\(C\_{t-1}\\)更新到\\(C\_t\\)，前面的两步已经把决策和数据都准备好了，现在只需将他们结合起来即可。

&ensp;&ensp;&ensp;将旧状态乘以\\(f\_t\\)，将状态中的信息需要忘记的去除掉。然后将得到的结果加上\\(i\_t * \tilde{C\_t}\\)，也就是将已经决定好要添加的信息添加到细胞状态上。

&ensp;&ensp;&ensp;回到语言模型，这一步的意思就是丢掉旧信息添加新信息。

![LSTM_step3](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/LSTM_step3.png?raw=true)

&ensp;&ensp;&ensp;最后，我们决定要输出什么。输出是基于细胞状态的，跟以前一样，先用\\(h\_{t-1}和x\_t\\)经过一个`sigmoid`来确定细胞状态的什么信息需要输出。将细胞状态经过一个`tanh`(将值限制在`-1`到`1`之间)后跟前面`sigmoid`输出相乘得到最终输出。因此我们输出了应该输出的东西。

&ensp;&ensp;&ensp;回到语言模型的例子，因为刚看到了一个主语，那么看完之后要输出对下一个词的预测有影响的信息，比如跟动词相关的信息。例如，可能输出主语的单复数相关的信息，这样下一个词拿到这些信息就能决定动词的形式。

![LSTM_step4](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/LSTM_step4.png?raw=true)

### LSTM变体

&ensp;&ensp;&ensp;上面我们描述的是最基础的`LSTM`，但是不是所有的`LSTM`都与上面的形式一样的。事实上，每篇使用`LSTM`的论文中`LSTM`架构都会有或多或少的不同。变化不大，但是它们中的一些还是值得一提。

&ensp;&ensp;&ensp;一种`LSTM`变体是来自[Gers & Schmidhuber (2000)](ftp://ftp.idsia.ch/pub/juergen/TimeCount-IJCNN2000.pdf)，其在基本`LSTM`的基础上加入了`peephole connections(猫眼连接)`，也就是在每个门的输入上都加上细胞状态。

![LSTM_v1](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/LSTM_v1.png?raw=true)

&ensp;&ensp;&ensp;上面在三个门上都加入了细胞状态，其实在一些论文中可能会只在某几个门上加。

&ensp;&ensp;&ensp;另一种变体是将`忘记门`和`输入门`耦合在一起。不再像原始`LSTM`那样分别决策要扔掉什么信息和添加什么信息到细胞状态，现在同时进行这两个决策。只有当要添加东西的时候才取扔掉旧东西，只有当要扔掉旧东西时才添加新东西。

![LSTM_v2](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/LSTM_v2.png?raw=true)

&ensp;&ensp;&ensp;一个更经典的变体是`GRU(Gated Recurrent Unit)`，来自[Cho, et al. (2014)](https://arxiv.org/pdf/1406.1078v3.pdf)，他将`忘记门`和`输入门`合并成一个`更新门`，将`细胞状态`和`隐藏状态`合并成一个状态，还做了一些其他的改的。`GRU`的结构比`LSTM`简单，很快得到广泛应用。

![LSTM_v3](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/LSTM_v3.png?raw=true)

&ensp;&ensp;&ensp;这就是一些比较著名的`LSTM`变体。当然还有很多其他的变体，像[Deep Gated RNNs](https://arxiv.org/pdf/1508.03790v2.pdf)。当然也有很多跟`LSTM`完全不同的结构来学习`长时间依赖`的，如[Clockwork RNNs](http://arxiv.org/pdf/1402.3511v1.pdf)
&ensp;&ensp;&ensp;哪些变体最好？这些变体的变化起作用吗？[Greff, et al. (2015)](http://arxiv.org/pdf/1503.04069.pdf)对流行的变体做了一个比较，发现他们都是相同的。[Jozefowicz, et al. (2015)](http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)对上万种`RNN`架构做了测试，发现一些模型在特定工作下的效果比`LSTM`好。

### 总结

&ensp;&ensp;&ensp;前面我提到的人民使用RNNs取得的显著效果中，大部分是使用`LSTM`实现的。这说明，它在大多数任务上的效果都会很好。 

&ensp;&ensp;&ensp;`LSTM`的原理写成一大堆公式很很吓人的，但是如果跟着本文一步一步的走下来，会很清楚的明白这些式子和`LSTM`的原理。

