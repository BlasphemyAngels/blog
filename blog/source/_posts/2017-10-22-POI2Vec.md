---
title: 位置推荐论文阅读：POI2Vec
date: 2017-10-23 10:18:44
categories:
  - papper
tags:
  - papper
  - AAAI2017
  - word2vec
---
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>
## 正文
&ensp;&ensp;&ensp;实验室的一些同学最近在搞位置推荐，为了跟他们有共同话题，就跟着他们阅读了一些论文，发现很有意思，一些思路和解决问题的思路历程很值得深思。
&ensp;&ensp;&ensp;下面就说一篇刚阅读完的一篇论文：`POI2Vec`。
[论文地址](http://www1.spms.ntu.edu.sg/~ymchee/papers/POI2Vec.pdf)
<!--more-->
### 预备知识
&ensp;&ensp;&ensp;由于本文的方法是从`word2vec`创新而来，所以在继续向下读之前务必要熟悉`word2vec`的原理。
[word2vec原理](http://augustineccl.com/2017/10/22/2017-10-20-wordembedding/)
### 摘要
&ensp;&ensp;&ensp;随着社交生活中位置信息数据的增加，关于POI（Point-of-Interest）的研究如用户移动行为建模和POI地点推荐等被广泛研究。最近，推荐用户下一个可能去的POI地点成为一个研究的热点，很多对此的研究论文已经发表。
&ensp;&ensp;&ensp;现在大多数已经存在的推荐系统研究注重于给用户推荐POI热点，很少有研究是给地点推荐用户。本文研究的课题是，给定一个POI热点，预测哪些用户将访问这个热点。
&ensp;&ensp;&ensp;文章提出一种隐含表示模型：POI2Vec，能够对地理因素进行建模。地理因素已经被证明为影响用户行为的一种重要因素。已有的模型对地理因素的建模不好。
&ensp;&ensp;&ensp;一个用户的行为是受他近期的签到行为和他的偏好影响的。但是用户的签到数据很稀疏，而且很难去对POI的连续转换和用户偏好进行建模。在NLP中，word2vector技术用来进行词嵌入，捕捉单词之间的序列语义关系。最近，word2vectpr被用在POI推荐中，但是存在两个问题，一个是不能捕捉POI的地理影响。地理因素在POI推荐中起着重要的作用，第二，它分别对序列影响和用户偏好分别建模，因为访问行为一般同时与序列影响和用户偏好有关，所以同时对两者进行建模会更合理。
&ensp;&ensp;&ensp;为了对地理因素进行建模，文章提出一个隐含表示模型，称为POI2Vec，在模型中，每个POI被表示为一个隐低维空间内的一个向量，向量的內积表示POI之间的联系。我们使用层次softmax进行学习隐含向量空间。对于层次softmax来说，很重要的一点是建立一个合适的二叉树。这里，不使用word2vector中使用的哈弗曼树，文章提出一种二叉树建立方法，能够对地理因素进行建模。我们层次化的对POI进行分割得到POI的一些不同区域。因为一个POI可能会对周围的POI进行影响，所以我们将一个POI放入多个周围的区域中。在二叉树中，一个POI可能会出现不只一次，这样能够捕捉到POI跟其他POI的丰富的关系。
### 相关工作
&ensp;&ensp;&ensp;已有方法：
* Collaborative Filtering(协同过滤)
* Factorization models(分解模型)
* Markov Chain(马尔科夫)
* Hidden Markov Chain(隐马尔科夫)
* Metric Embedding(矩阵嵌入)
&ensp;&ensp;&ensp;文章的工作与现有工作至少有两点不同：
* 从一个不同的视角考虑一个新的任务：找到一个POI在接下来的几个小时内的潜在用户。
* 提出的模型同时学习序列影响和用户偏好。
### 问题定义
&ensp;&ensp;&ensp;用户集合表示为U，POI集合表示为L，每个POI地点l的地理表示为\\(\langle l^{Lat},l^{Lon}\rangle\\)。用H表示用户的签到数据集，每次签到(u,l,t)表示用户u在时间t时访问地点l。
&ensp;&ensp;&ensp;下面给出问题的定义：
&ensp;&ensp;&ensp;给定用户集合U和POI集合L，并指定时间t和时间阈值\\(\tau\\)，预测在时间范围\\((t,t+\tau)\\)内有哪些用户会访问地点l。
### POI2Vec 表示模型
#### POI2Vec对连续序列影响建模
&ensp;&ensp;&ensp;先用POI2Vec对POI地点的序列影响进行建模。
&ensp;&ensp;&ensp;POI2Vec思路很简单，就是将word2vec用于POI，动机是：作者观察到签到数据中的POI分布和文本中word的分布很类似。
&ensp;&ensp;&ensp;给定用户u和他当前的地点\\(l\_c^u\\)，用\\(C(l\_c^u)\\)代表用户u在访问\\(l\_c^u\\)之前的一定时间范围内访问的所有地点，即\\(C(l\_c^u)=\lbrace l\_i^u;0<\Delta(l\_i^u,l\_c^u)<\tau\rbrace\\)。\\(\Delta(l\_i^u,l\_c^u)\\)代表\\(l\_i^u\\)和\\(l\_c^u\\)之间的时间长度。目标是得到给定其POI地点的访问上下文之后，预测会访问当前POI的概率。
&ensp;&ensp;&ensp;将每个POI表示成一个向量\\(w(l)\in R^D\\)，采用CBOW(Continuous Bag-of-Words,连续词袋模型)对其进行建模。
&ensp;&ensp;&ensp;则概率Pr(l|C(l))为：
$$
Pr(l|C(l))=\frac{e^{(w(l)\cdot\phi(C(l)))}}{\sum\_{l\_i\in L}e^{w(l\_i)\cdot\phi(C(l))}}
$$
&ensp;&ensp;&ensp;其中：
$$
\phi(C(l))=\sum\_{l\_c\in C(l)}w(l\_c)
$$
&ensp;&ensp;&ensp;看到上面的式子是不是感觉很熟悉，其实它就是`softmax的POI版本`（仔细思考这句话）。
&ensp;&ensp;&ensp;同word2vec一样当POI数量很大时计算上面的式子的时间复杂度太高。所以需要优化，优化一般使用`层次softmax`。层次softmax的关键步骤是建立二叉树，在word2vec中是通过单词的频率建立哈弗曼树，而在此文章中，为了考虑POI的地理因素，作者提出了一种创建二叉树的方法，具体看下一节。
#### 加入地理因素
&ensp;&ensp;&ensp;地理因素在POI推荐中起着重要作用，那么怎么在上面提到的层次softmax建二叉树过程中加入地理因素呢？一个矩形每次将上次得到的区域二等分，反复分割就得到了\\(2^n\\)个矩形，这样的分割方式可以形成一个二叉树。 先区域分割，分割成一个一个的小区域，注意在分区域时，每次等分，最后分割成一个一个同样大小的矩形，矩形的边长记为\\(\theta\\)，这样每一个POI地点都属于一个区域。在同一个区域内的POI点被视为`很接近`。但是有的时候不同区域的POI在距离上也是相近的，那么怎么办呢？使得一个POI可以属于多个区域，以一个POI为中心，做一个\\(\theta\times\theta\\)的矩形，如果此矩形与其他区域重叠，则将此POI赋于到这些重叠的区域中。具体如下图所示
![POI2Vec_g](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/poi2vec_g.png?raw=true)
&ensp;&ensp;&ensp;如图将区域分成\\(R\_{00},R\_{01},R\_{10},R\_{11}\\)四个区域，那么\\(l\_1\\)属于\\(R\_{01}\\)，\\(l\_2\\)属于\\(R\_{11}\\)，\\(l\_4\\)属于\\(R\_{00}\\)，\\(l\_3\\)属于\\(R\_{10}\\)，但是可以从图中明显可以看出\\(l\_1和l\_2\\)也是很近的，这个时候就需要将一个POI点赋于多个区域了，按照上面的方法，分别以\\(l\_1和l\_2\\)为中心画出长度为\\(\theta\\)的矩形（图中红色边框的矩形）。可以看到以\\(l\_1和l\_2\\)为中心的矩形跟区域\\(R\_{01}和R\_{11}\\)都有重叠，所以将\\(l\_1\\)加入到\\(R\_{11}\\)区域中，将\\(l\_2\\)加入到\\(R\_{01}\\)区域中，其他的点类似。
&ensp;&ensp;&ensp;很明显可以发现，每个POI可能属于的区域数目为1、2或4个。
&ensp;&ensp;&ensp;为了后面创建哈弗曼树，文章定义了一个POI属于一个区域的概率：以POI为中心的矩形与区域重叠部分的面积占区域总面积的比例。为了后续使用，将这个概率记为：\\(Pr(R\_i)^l\\)，\\(R\_i\\)代表编号为i的区域。
&ensp;&ensp;&ensp;然后就是创建层次softmax的二叉树了，先使用区域创建顶层二叉树，如下图蓝线上面的树，可以发现这是一颗完全二叉树，树的叶子结点就是最小的区域。其实这棵树就代表区域的分割过程。然后以创建好的二叉树的叶子结点为根分别创建哈弗曼树，哈弗曼树创建的依据在此POI上用户们的签到频率。
![POI2Vec_tree](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/POI2Vec_tree.png?raw=true)
&ensp;&ensp;&ensp;总结一下，本文提出的POI2Vec模型有两个优点，一个是它考虑了地理因素的影响，在同一个区域的POI在地理上是相近的，二是在创建的二叉树中，每个POI可能有多条路径，这样会使得模型效果更好，因为一个POI跟其他POI产生很多关系会被充分捕捉。
#### 概率估计
&ensp;&ensp;&ensp;现在来定义一下各个结果概率：
&ensp;&ensp;&ensp;跟在word2vec中一样，在二叉树的非叶子结点上是可训练的参数，最终从跟到树的叶子计算的概率就是在当前上下文下访问l的概率：
$$
path=(b\_0^l,b\_1^l,...,b\_n^l)\\\\
Pr(l|C(l))^{path}=\prod\_{b\_i^l\in path}Pr(b\_i^l|\phi(C(l)))\\\\
Pr(b\_i|\phi(C(l))=sigmoid(\psi(b\_i^l)\phi(C(l)))\\\\
$$
&ensp;&ensp;&ensp;其中\\(\psi(b\_i^l)\in R^D\\)代表每个结点上的参数向量。
&ensp;&ensp;&ensp;但是一个POI在二叉树中可能有多条路径，因此需要将它们结合起来计算概率：
$$
Pr(l|C(l))=\prod\_{path\_k\in P(l)}Pr(path\_k)\cdot Pr(l|C(l))^{path\_k}
$$
&ensp;&ensp;&ensp;其中\\(P(l)\\)代表POI点l在二叉树中所有路径的集合。
&ensp;&ensp;&ensp;如果\\(path\_k\\)为l的路径，而l属于区域\\(R\_x\\)，那么\\(Pr(path\_k)=Pr(R\_x)^l\\)，而\\(Pr(R\_x)^l\\)就是POI属于某一个区域的概率，上面定义过它的计算。
#### 参数学习
&ensp;&ensp;&ensp;学习目标为最大化得到所有POI点的后验概率。
$$
\Theta=arg\min\_{\Theta}\prod\_{(l,C(l))\in H}Pr(l|C(l))
$$
&ensp;&ensp;&ensp;其中\\(\Theta\\)是一些可学习的参数。
### 将POI2Vec用于建模用户偏好
&ensp;&ensp;&ensp;前面已经使用POI2Vec对连续POI序列预测下一个POI问题进行建模，现在再加入用户偏好。
&ensp;&ensp;&ensp;跟前面的POI概率定义相似，用户u访问POI点l的概率为：
$$
Pr(l|u)=\frac{e^{w(l)\cdot x(u)}}{Z(u)}
$$
&ensp;&ensp;&ensp;其中\\(Z(u)=\sum\_{l\_i\in L}e^{(w(l\_i)\cdot x(u))}\\)，同样的Pr(l|u)也可以使用上面创建好的层次softmax树计算，而且可以使用同一棵树训练计算（这样就同时对连续POI序列和用户偏好进行建模了）。
&ensp;&ensp;&ensp;如果在计算时既有连续POI上下文也有用户偏好数据，并假设它们相互独立，那么：
$$
Pr(l|u,C(l))=Pr(l|u)xPr(l|C(l))
$$
&ensp;&ensp;&ensp;综上所述，用户在某一时间访问某一地点的概率为：
$$
Pr(u,l,t)=
\begin{cases}
Pr(l|u,C(l))\quad if\ C(l)\ exists\\\\
Pr(l|u)\quad\quad\quad\quad otherwise
\end{cases}
$$
&ensp;&ensp;&ensp;目标函数为：
$$
\Theta=arg\max\_{\Theta}\prod\_{(u,l,t)\in H}Pr(u,l,t)
$$
### 预测潜在访问用户
&ensp;&ensp;&ensp;用户u在不久后访问l的似然概率为\\(F(x(u)\cdot w(l),w(l\_c)\cdot w(l))\\)，\\(x(u)\cdot w(l)\\)反映用户偏好，\\(w(l\_c)\cdot w(l)\\)反映序列影响。\\(F\\)是聚合函数，一般聚合函数可以使用`Max`函数或者`Sum`函数。
&ensp;&ensp;&ensp;对于每个用户计算他的分数：
$$
s(u,l)=
\begin{cases}
F((x(u)\cdot w(l),w(l\_c)\cdot w(l)))\quad with\ recent\ positions\\\\
x(u)\cdot w(l)\quad\quad\quad\quad\quad\quad\quad\quad\quad otherwise
\end{cases}
$$
&ensp;&ensp;&ensp;根据分数找到Top-k个用户即可。
### 实验
&ensp;&ensp;&ensp;实验结果：
![POI2Vec_exp](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/POI2Vec_exp.png?raw=true)
* FMC: the factorized Markov chain model
* ME: Metric Embedding model
* NS: the negative sampling technique for word2vec
* HS: conventional hierarchical softmax with Huffman tree
* FPMC: factorizing personalized Markov chains
* PRME: personalized ranking metric embedding
* CWRAP: explore the context of locations to model user preference
* U: only utilize user preference to predict potential visitors.
* URP: only consider users with recentpositions.
* MAX: Max aggregation function is used
* SUM: utilize the Sum aggregation function
