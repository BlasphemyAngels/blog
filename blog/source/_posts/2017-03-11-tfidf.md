---
layout: post
title: "TF-IDF算法"
date: 2017-03-11
categories:
  - nlp
tags: 
  - NLP
  - 提取关键词
---


## 正文

### 定义

term frequency-inverse document frequency 是一种用于抽取一个语料库中一个文件的关键词，从而得到一个词对于语料库中的一个文件的重要程度。

<!--more-->

### 原理

#### TF

首先思考，对于一篇文章而言，什么词能成为关键词呢？可以很快考虑到，词频，即出现次数多的词，有很大概率是关键词。<br/>

A=某个词在文档中的次数<br/>

B=文章的总次数<br/>

C=文档中出现最多的词的次数<br/>

<img src="http://chart.googleapis.com/chart?cht=tx&chl= TF=\frac{A}{B}" style="border:none;"> 

或<br/>

<img src="http://chart.googleapis.com/chart?cht=tx&chl= TF=\frac{A}{C}" style="border:none;"> 


#### IDF

有了词频之后，可以发现，在其中，“的”，“我”, “你”等词出现的次数最多。然而显然这些词并不是关键词，那么怎么办？

可以发现，只是词频最大的单词并不一定是关键词，有些词出现次数少一些，但是它比较重要，所以是关键词，所以应该找一个标准来定义词的重要度。

可以认同的是，如果一个词在别的文章中出现次数较少，而在本文中多次出现，那么这个词重要性就比较高。这就是逆文档概率(IDF)。


D=语料库的总文档数<br/>
E=包含该词的文档数+1

<img src="http://chart.googleapis.com/chart?cht=tx&chl= $$IDF=log(\frac{D}{E})$$" style="border:none;"> 

#### 结论

有了TF和IDF,那么只要把他们相乘就能得到关键词的重要程度了<br/>

`TF-IDF = TF x IDF`

### 总结

TF-IDF算法的优点是简单快速，结果比较符合实际情况。缺点是，单纯以"词频"衡量一个词的重要性，不够全面，有时重要的词可能出现次数并不多。而且，这种算法无法体现词的位置信息，出现位置靠前的词与出现位置靠后的词，都被视为重要性相同，这是不正确的。

## 参考链接:
[TF-IDF与余弦相似性的应用（一）：自动提取关键词
](http://www.ruanyifeng.com/blog/2013/03/tf-idf.html)


<br>

转载请注明:[Artemis的博客]([https://BlasphemyAngels.github.io)--> [点此看原文
](https://blasphemyangels.github.io/2017/03/tfidf/)
