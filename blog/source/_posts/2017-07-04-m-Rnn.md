---
title: m-Rnn
date: 2017-07-04 14:45:23
tags:
---

[论文地址](https://arxiv.org/pdf/1412.6632v5.pdf)

文章提出一种多模态图像文本匹配的方法，使用CNN-RNN架构，如下图。

![m-rnn](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/m-rnn.png?raw=true)

<!--more-->

其中前两层是词嵌入层，讲每一个词变为一个稠密向量。接下来是recurrent层，用来捕捉句子的语义和语法信息。这里注意训练好词向量之后不是将它们拼接起来而是输入到recurrent层，用来捕捉句子的语义和语法信息。

![m-rnn-rt](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/m-rnn-rt.png?raw=true)

我们先将上一个状态r(t-1)映射到词向量空间内w(t),然后再加上w(t)。

下一层是多模态层，将文本向量，图像向量和recurrent层输出向量映射成为到一个统一空间。

![m-rnn-m](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/m-rnn-m.png?raw=true)

再相加得到多模态的输出。

其中g函数使用如下：

![m-rnn-g](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/m-rnn-m.png?raw=true)

[公式原文](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)

将值映射到一个相比双曲正切更加非线性的范围，而且收敛速度更快。

最后一层是softmax层，输出概率。

![loss](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/m-rnn-loss.png?raw=true)

其中L代表句子长度，wi代表词，I代表图像。

![loss2](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/m-rnn-loss2.png?raw=true)

N代表训练集单词数，Ns代表训练集中句子的个数。

最小化cost相当于最大化由图像产生句子的最大概率。

### 数据集

* IAPR TC-12 20000张图片，每张图片评价1.7个描述，每张图片至少一个描述。
* Flickr8K 8000张图片，每张图片5个描述，6000-train 1000-test 1000-vaildate
* Flickr30K 31783张图片，每张图片5个描述。
* MS COCO 82783张图片，每张图片5个描述。

### 部分实验结果

![exper](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/m-rnn-exper.png?raw=true)

### 总结

m-rnn是一个先生成后检索模型，其应该是第一个使用 CNN + RNN 这种 encoder-decoder 模型来做图文相关任务.

* 使用 VggNet 来抽取图片特征
* 使用两个 Embedding 层来对每个单词进行稠密特征编码
* 多模态部分的输入有图片特征、单词编码和上下文信息
* 目标函数是给定图片 I，使得生成的图片描述尽可能像图片的真实描述 S 。

* image的特征并没有输入到RNN
