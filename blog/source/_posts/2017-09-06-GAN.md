---
title: GAN初涉
date: 2017-09-07 21:46:42
copyright: true
mathjax: true
categories:
  - DL
tags:
  - GAN
---

## 正文

&ensp;&ensp;&ensp;论文原文[Generative Adversarial Nets](https://arxiv.org/abs/1406.2661)

&ensp;&ensp;&ensp;生成对抗网络是近几年研究火热的方向，为了赶上潮流，本人最近也要开始研究(咳咳，有点把科研说的俗了QAQ)。

&ensp;&ensp;&ensp;生成对抗网络是一种生成式模型，使用一个对抗过程训练网络，而不再像已有方式那样使用复杂的马尔科夫等求解。

&ensp;&ensp;&ensp;那么问题来了，什么是生成式模型呢。生成模型是相对判别模型来说的。判别模型很简单，机器学习算法的作用一般是给定样本x，得到其所在的类别(分类)、预测结果（回归）。而学习这个过程的不同分为生成模型和判别模型。

<!--more-->

### 判别模型

&ensp;&ensp;&ensp;如果是通过学习样本x和结果y之间的条件概率P(y|x)或者y=f(x)的方式称为判别模型。事实上，y=f(x)的隐含也是学习了P(y|x)。因为你仔细想想，使用y=f(x)的意思就是根据x学习一个函数f将x映射到y，而f是根据数据得到的，在这个学习f的过程中不难想到，实际上是学习到了P(y|x)然后得到f函数最终将x映射到y。

![panbie](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/shengcheng.png?raw=true)

### 生成模型

&ensp;&ensp;&ensp;如果是通过学习x和y的联合分布律从而进行得到x到y的映射方式称为生成模型。使用公式:<img src="http://chart.googleapis.com/chart?cht=tx&chl= P(y|x)=\frac{P(x, y)}{P(x)}}" style="border:none;">得到。

![shengcheng](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/panbie.png?raw=true)

&ensp;&ensp;&ensp;现在生成模型主要有两种用法：

* Density estimation(密度评估)

![gan-f1](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/gan-f1.png?raw=true)

&ensp;&ensp;&ensp;如图所示，简单的来说，密度估计就是估计数据概率分布。

* Sample generation(采样生成)

![gan-f2](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/gan-f2.png?raw=true)

&ensp;&ensp;&ensp;采样生成则是根据应有的数据，生成跟已有的数据同分布的新数据。


### 应用

&ensp;&ensp;&ensp;生成模型的应用有很多，如：

![gan-y-1](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/gan-y-1.png?raw=true)

&ensp;&ensp;&ensp;根据视频前面的帧，预测生成下一个帧。

![gan-y-2](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/gan-y-2.png?raw=true)

&ensp;&ensp;&ensp;提高图像的分辨率

![gan-y-3](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/gan-y-3.png?raw=true)

&ensp;&ensp;&ensp;场景生成或根据物体轮廓图生成物体的整个图像。

### 生成模型原理

&ensp;&ensp;&ensp;生成模型的原理总的来说，就是通过最大化已知数据的似然概率来确定模型的隐含参数，也就是最大似然估计的过程。

![MLE](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/MLE.png?raw=true)

### 生成模型族谱

![shengchengs](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/shengchengs.png?raw=true)

&ensp;&ensp;&ensp;下面是我从网上抄来的一些现有生成模型的总结：

* Fully Visible Belief Nets

![FVBN](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/FVBN.png?raw=true)

* WaveNet

![WN](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/WN.png?raw=true)

* Change of Variables

![COV](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/COV.png?raw=true)

* Variational Autoencoder

![VA](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/VA.png?raw=true)

* Boltzmann Machines

![BM](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/BM.png?raw=true)

### 生成对抗网络

&ensp;&ensp;&ensp;好了介绍了这么多，终于来到生成对抗网络了。

![ANF](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/ANF.png?raw=true)

&ensp;&ensp;&ensp;生成对抗网络总体原理和生成网络是一致的，也是使用极大似然估计的方式，但是它的好处是使用对抗方式完成。

&ensp;&ensp;&ensp;生成对抗网络由一个生成器和一个判别器组成。生成器生成跟真实数据分布一致的数据，而判别器则判别数据真实数据还是由判别器生成的，生成器的目标是生成更接近真实分布的数据而不被判别器所判别，而判别器的目标是判别出真实数据和生成器生成数据的不同。这样两者就组成了一个零和博弈，生成器生成越来越接近真实数据的分布，从而使得判别器不能判别出数据是来自生成器还是真实数据，这时判别器就需要提高自己的判别能力，而判别器能力的提高，又会使得生成器提高自己的能力以生成更接近真实分布的数据而不被判别器所识别。最终两者达到一个平衡点，即生成器和判别器的能力再也不能提高，这个平衡点在博弈论被称为纳什均衡。

#### 博弈

&ensp;&ensp;&ensp;下面就介绍一下一些博弈论的知识。

&ensp;&ensp;&ensp;所谓零和博弈，零和博弈（zero-sum game），又称零和游戏，与非零和博弈相对，是博弈论的一个概念，属非合作博弈。指参与博弈的各方，在严格竞争下，一方的收益必然意味着另一方的损失，博弈各方的收益和损失相加总和永远为“零”，双方不存在合作的可能。 ----转自[百度百科](https://baike.baidu.com/item/%E9%9B%B6%E5%92%8C%E5%8D%9A%E5%BC%88/3562463?fr=aladdin)
&ensp;&ensp;&ensp;而所谓纳什均衡则是指一个博弈游戏的参与人所采用的这样一个策略组合，在该策略组合上任何参与人单独改变策略都不会得到好处。也就是说，如果在一个策略组合上，当所有其他人都不改变策略时，没有人会改变自己的策略（如果改变，则损失收益），则该策略组合就是一个纳什均衡。

&ensp;&ensp;&ensp;下面是我从网上搜集的一些纳什均衡的案例可以用来辅助理解纳什均衡。

* 智猪博弈

&ensp;&ensp;&ensp;猪圈里有两头猪，一头大猪，一头小猪。猪圈的一边有个踏板，每踩一下踏板，在远离踏板的猪圈的另一边的投食口就会落下少量的食物。如果有一只猪去踩踏板，另一只猪就有机会抢先吃到另一边落下的食物。当小猪踩动踏板时，大猪会在小猪跑到食槽之前刚好吃光所有的食物；若是大猪踩动了踏板，则还有机会在小猪吃完落下的食物之前跑到食槽，争吃到另一半残羹。

&ensp;&ensp;&ensp;那么，两只猪各会采取什么策略？答案是：小猪将选择“搭便车”策略，也就是舒舒服服地等在食槽边；而大猪则为一点残羹不知疲倦地奔忙于踏板和食槽之间。

&ensp;&ensp;&ensp;原因何在？因为，小猪踩踏板将一无所获，不踩踏板反而能吃上食物。对小猪而言，无论大猪是否踩动踏板，不踩踏板总是好的选择。反观大猪，已明知小猪是不会去踩动踏板的，自己亲自去踩踏板总比不踩强吧，所以只好亲力亲为了。

* 囚徒困境

&ensp;&ensp;&ensp;（1950年，数学家塔克任斯坦福大学客座教授，在给一些心理学家作讲演时，讲到两个囚犯的故事。）

&ensp;&ensp;&ensp;假设有两个小偷A和B联合犯事、私入民宅被警察抓住。警方将两人分别置于不同的两个房间内进行审讯，对每一个犯罪嫌疑人，警方给出的政策是：如果一个犯罪嫌疑人坦白了罪行，交出了赃物，于是证据确凿，两人都被判有罪。

&ensp;&ensp;&ensp;如果另一个犯罪嫌疑人也作了坦白，则两人各被判刑8年；如果另一个犯罪嫌人没有坦白而是抵赖，则以妨碍公务罪（因已有证据表明其有罪）再加刑2年，而坦白者有功被减刑8年，立即释放。如果两人都抵赖，则警方因证据不足不能判两人的偷窃罪，但可以私入民宅的罪名将两人各判入狱1年。
<center><h4><font color="red">囚徒困境</font></h4></center>

A/B   |  坦白 | 不坦白
------|-------|----
坦白  | -8,-8 | 0,-10
不坦白| -10,0 | -1,-1

&ensp;&ensp;&ensp;关于案例，显然最好的策略是双方都抵赖，结果是大家都只被判1年。但是由于两人处于隔离的情况，首先应该是从心理学的角度来看，当事双方都会怀疑对方会出卖自己以求自保、其次才是亚当·斯密的理论，假设每个人都是“理性的经济人”，都会从利己的目的出发进行选择。

&ensp;&ensp;&ensp;这两个人都会有这样一个盘算过程：假如他坦白，如果我抵赖，得坐10年监狱，如果我坦白最多才8年；假如他要是抵赖，如果我也抵赖，我就会被判一年，如果我坦白就可以被释放，而他会坐10年牢。综合以上几种情况考虑，不管他坦白与否，对我而言都是坦白了划算。两个人都会动这样的脑筋，最终，两个人都选择了坦白，结果都被判8年刑期。

&ensp;&ensp;&ensp;基于经济学中Rational agent的前提假设，两个囚犯符合自己利益的选择是坦白招供，原本对双方都有利的策略不招供从而均被判处一年就不会出现。这样两人都选择坦白的策略以及因此被判8年的结局，纳什均衡首先对亚当·斯密的“看不见的手”的原理提出挑战：按照斯密的理论，在市场经济中，每一个人都从利己的目的出发，而最终全社会达到利他的效果。但是我们可以从“纳什均衡”中引出“看不见的手”原理的一个悖论：从利己目的出发，结果损人不利己，既不利己也不利他。

* 普通范式博弈

&ensp;&ensp;&ensp;GOO公司和SAM公司是某手机产品生态的两大重量级参与者，双方在产业链的不同位置上各司其职且关系暧昧，有时也往往因商业利益和产品影响力的争夺而各怀异心。二者的收益也随着博弈的变化而不断更替。

GOO/SAM| 合作 | 背叛
-------|------|-----
 合作  | 3, 3 | -3, 5
 背叛  | 5,-3 | -1,-1

&ensp;&ensp;&ensp;上图表格模拟了两家公司的博弈现状，双方各有两个可选策略“合作”与“背叛”，格中的四组数据表示四个博弈结局的分数（收益），每组数据的第一个数字表示GOO公司的收益，后一个数字表示SAM公司的收益。

&ensp;&ensp;&ensp;博弈是同时进行的，一方参与者必须站在对方的角度上来思考我方的策略选择，以追求收益最大化。这在博弈论里称作Putting yourselves into other people's shoes。

&ensp;&ensp;&ensp;现在我们以GOO公司为第一人称视角来思考应对SAM公司的博弈策略。假如SAM公司选择合作，那么我方也选择合作带来的收益是3，而我方选择背叛带来的收益是5，基于理性的收益最大化考虑，我方应该选择背叛，这叫严格优势策略；假如SAM公司选择背叛，那么我方选择合作带来的收益是-3，而选择背叛带来的收益为-1，为使损失降到最低，我方应该选择背叛。最后，GOO公司的分析结果是，无论SAM公司选择合作还是背叛策略，我方都必须选择背叛策略才能获得最大化的收益。

&ensp;&ensp;&ensp;同理，当SAM公司也以严格优势策略来应对GOO公司的策略选择时，我们重复上述分析过程，就能得出结论：无论GOO公司选择合作还是背叛策略，SAM公司都必须选择背叛策略才能获得最大化收益。

&ensp;&ensp;&ensp;最后我们发现，本次博弈的双方都采取了背叛策略，各自的收益都为-1，这是一个比较糟糕的结局，尽管对任何一方来说都不是最糟糕的那种。这种局面就是著名的“囚徒困境”。

&ensp;&ensp;&ensp;但是，博弈的次数往往不止一次，就像COO与SAM公司双方的商业往来也许会有很多机会。当二者经历了多次背叛策略的博弈之后，发现公式上还有一个（3，3）收益的双赢局面，这比（-1，-1）的收益结果显然要好很多，因此二者在之后的博弈过程中必然会尝试互建信任，从而驱使双方都选择合作策略。

&ensp;&ensp;&ensp;这里有一个理想化假设，那就是假设双方都知道博弈次数是无限的话，也就是说双方的商业往来是无止尽的，那么二者的策略都将持续选择合作，最终的博弈收益将定格在（3，3），这就是一个纳什均衡。既然博弈次数是无限的，那么任何一方都没有理由选择背叛策略去冒险追求5点短暂收益，而招致对方在下一轮博弈中的报复（这种报复在博弈论里称作“以牙还牙”策略）。

&ensp;&ensp;&ensp;还有另一种假设情况是，假使双方都知道博弈次数是有限的，也许下一次博弈就是最后一次，那么为了避免对方在最后一轮博弈中选择背叛策略而使我方遭受-3的收益损失，于是双方都重新采取了背叛的策略选择，最后的博弈结果又回到了（-1，-1），这就形成了第二个纳什均衡。

&ensp;&ensp;&ensp;由此可见，随着次数（博弈性质）的变化，纳什均衡点也并非唯一，这在下一个例子中有着更明显的表现。

* 饿狮博弈

&ensp;&ensp;&ensp;题设为A、B、C、D、E、F六只狮子（强弱从左到右依次排序）和一只绵羊。假设狮子A吃掉绵羊后就会打盹午睡，这时比A稍弱的狮子B就会趁机吃掉狮子A，接着B也会午睡，然后狮子C就会吃掉狮子B，以此类推。那么问题来了，狮子A敢不敢吃绵羊？

![eshi](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/eshi?raw=true)

&ensp;&ensp;&ensp;为简化说明，我们先给出此题的解法。该题须采用逆向分析法，也就是从最弱的狮子F开始分析，依次前推。假设狮子E睡着了，狮子F敢不敢吃掉狮子E？答案是肯定的，因为在狮子F的后面已没有其它狮子，所以狮子F可以放心地吃掉午睡中的狮子E。

&ensp;&ensp;&ensp;继续前推，既然狮子E睡着会被狮子F吃掉，那么狮子E必然不敢吃在他前面睡着的狮子D。

&ensp;&ensp;&ensp;再往前推，既然狮子E不敢吃掉狮子D，那么D则可以放心去吃午睡中的狮子C。依次前推，得出C不吃，B吃，A不吃。所以答案是狮子A不敢吃掉绵羊。
&ensp;&ensp;&ensp;细心的人也许会发现，假如增加或减少狮子的总数，博弈的结果会完全不同。我们用下图来验证：

![eshi2](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/eshi2?raw=true)

&ensp;&ensp;&ensp;我们在狮子F的后面增加了一只狮子G，总数变成7只。用逆向分析法按照上题步骤再推一次，很容易得出结论：狮子G吃，狮子F不吃，E吃，D不吃，C吃，B不吃，A吃。这次的答案变成了狮子A敢吃掉绵羊。

&ensp;&ensp;&ensp;对比两次博弈我们发现，狮子A敢不敢吃绵羊取决于狮子总数的奇偶性，总数为奇数时，A敢吃掉绵羊；总数为偶数时，A则不敢吃。因此，总数为奇数和总数为偶数的狮群博弈结果形成了两个稳定的纳什均衡点。

* 硬币正反

&ensp;&ensp;&ensp;你正在图书馆枯坐，一位陌生美女主动过来和你搭讪，并要求和你一起玩个数学游戏。美女提议：“让我们各自亮出硬币的一面，或正或反。如果我们都是正面，那么我给你3元，如果我们都是反面，我给你1元，剩下的情况你给我2元就可以了。”那么该不该和这位姑娘玩这个游戏呢？这基本是废话，当然该。问题是，这个游戏公平吗？

&ensp;&ensp;&ensp;每一种游戏依具其规则的不同会存在两种纳什均衡，一种是纯策略纳什均衡，也就是说玩家都能够采取固定的策略(比如一直出正面或者一直出反面)，使得每人都赚得最多或亏得最少；或者是混合策略纳什均衡，而在这个游戏中，便应该采用混合策略纳什均衡。

  你/美女 | 美女出正面 | 美女出反面
----------|------------|------------
 你出正面 | 3,-3       | -2, 2
 你出反面 | -2, 2       | +1,-1

&ensp;&ensp;&ensp;假设我们出正面的概率是x，反面的概率是1-x，美女出正面的概率是y，反面的概率是1-y。为了使利益最大化，应该在对手出正面或反面的时候我们的收益都相等，由此列出方程就是

&ensp;&ensp;&ensp;`3x + (-2)*(1-x)=(-2) * x + 1*( 1-x )`解方程得`x=3/8`。

&ensp;&ensp;&ensp;同样，美女的收益，列方程

&ensp;&ensp;&ensp;`-3y + 2( 1-y)= 2y+ (-1) * ( 1-y)`

&ensp;&ensp;&ensp;解得y也等于3/8，而美女每次的期望收益则是`2(1-y)- 3y = 1/8`元。这告诉我们，在双方都采取最优策略的情况下，平均每次美女赢1/8元。

&ensp;&ensp;&ensp;其实只要美女采取了(3/8,5/8)这个方案，不论你再采用什么方案，都是不能改变局面的。如果全部出正面，每次的期望收益是 `(3+3+3-2-2-2-2-2)/8=-1/8`元；如果全部出反面，每次的期望收益也是`(-2-2-2+1+1+1+1+1)/8=-1/8`元。而任何策略无非只是上面两种策略的线性组合，所以期望还是-1/8元。但是当你也采用最佳策略时，至少可以保证自己输得最少。否则，你肯定就会被美女采用的策略针对，从而赔掉更多。

#### 模型

&ensp;&ensp;&ensp;对于生成器来说，通过将有特定分布的噪音<img src="http://chart.googleapis.com/chart?cht=tx&chl= z=p_z(z)}" style="border:none;">通过一个可微的函数<img src="http://chart.googleapis.com/chart?cht=tx&chl= G(z;\theta_g)}" style="border:none;">映射为输出x，这里的这个函数G就是生成器，如下式：

![generator](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/generator.png?raw=true)

* 函数必须可微，以便可以求梯度进行学习进行学习
* 没有可逆性的必要
* 可以对任何维度的z进行训练
* z的维度有时需要大于x

&ensp;&ensp;&ensp;同样对于判别器来说，其也是一个可微函数<img src="http://chart.googleapis.com/chart?cht=tx&chl= D(x;\theta_d)}" style="border:none;">通过对输入的数据x进行判别是来自生成器<img src="http://chart.googleapis.com/chart?cht=tx&chl= p_g}" style="border:none;">     还是来自真实数据<img src="http://chart.googleapis.com/chart?cht=tx&chl= p_data}" style="border:none;">

&ensp;&ensp;&ensp;函数G和D都可以使用神经网络实现，如MLP(多层感知机)等。

#### 训练过程

&ensp;&ensp;&ensp;我们训练D使得它能够正确区分来自生成器的数据和真实数据，训练G使得生成器的数据更接近真实数据。

&ensp;&ensp;&ensp;假设已经给定G，优化D，那么根据最小化交叉熵损失写出目标函数为：

![loss](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/loss.png?raw=true)

&ensp;&ensp;&ensp;其实就是一个最大最小游戏。

<img src="http://chart.googleapis.com/chart?cht=tx&chl= J^{(D)}=-\frac{1}{2}\int_xp_{data}(x)logD(x)-\frac{1}{2}\int_zp_z(z)log(1-D(G(z)))dz" style="border:none;">
<img src="http://chart.googleapis.com/chart?cht=tx&chl= =-\frac{1}{2}\int_x[p_{data}(x)logD(x))+p_g(x)log(1-D(x))]dx}" style="border:none;">

&ensp;&ensp;&ensp;对于任何非零实数m,n，且实数值y在范围[0,1]之间，那么：

<img src="http://chart.googleapis.com/chart?cht=tx&chl= -mlog(y)-nlog(1-y)" style="border:none;">

&ensp;&ensp;&ensp;在m/m+n处取得最小值，于是在给定G的条件下，在

<img src="http://chart.googleapis.com/chart?cht=tx&chl= D_G^*=\frac{p_{data}(x)}{p_{data}(x)+p_g(x)}" style="border:none;">

&ensp;&ensp;&ensp;处取得最小值，这里也能表示GAN估计的是两个概率分布的比值。

&ensp;&ensp;&ensp;当真实分布和生成器分布相同时，两者等于二分之一，此时D为1/2，D的损失函数的梯度为0，无法继续优化。达到均衡。

&ensp;&ensp;&ensp;上面已经讨论了当生成器分布和真实数据一致时，判别器达到最优，那么此时生成器是否也是最优呢？

&ensp;&ensp;&ensp;D的训练目标可以看做最大化判别似然概率P(Y=y|x)，其中Y代表x是来自真实数据(y=1)还是来自生成器数据(y=0)。因此：

![cg](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/cg.png?raw=true)

<img src="http://chart.googleapis.com/chart?cht=tx&chl= p_g=p_{data}, D_G^*(x)=\frac{1}{2}}" style="border:none;">

&ensp;&ensp;&ensp;将上式带入上面公式C(G)中，得：

<img src="http://chart.googleapis.com/chart?cht=tx&chl= C(G)=log\frac{1}{2}+log\frac{1}{2}=-log4" style="border:none;">

&ensp;&ensp;&ensp;再将`-log4`从`C(G)`中减去，并将原始`C(G)`使用`KL距离`表示，得：

![cg2](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/cg2.png?raw=true)

观察上面式子的后两项，其结果可以用`JS散度`表示：

![cg3](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/cg3.png?raw=true)

&ensp;&ensp;&ensp;由于两个分布的`JS散度`值是非负的，而且当且仅当两者相等时取0，所以`C(G)`的全局最小值为`-log4`，此时两个分布相同，即：

<img src="http://chart.googleapis.com/chart?cht=tx&chl= p_g=p_{data}" style="border:none;">

&ensp;&ensp;&ensp;一般来说，训练时对D更新k次，再对G更新一次。

![process](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/procesw.png?raw=true)

&ensp;&ensp;&ensp;如上图所示，蓝色的线代表判别器，绿色的线代表生成器，黑色的线代表真实数据，由上面图从左到右的渐变过程可以发现，GAN是逐步将生成器分布拟合到真实数据分布上，最终判别器不再发生变化，达到均衡。

&ensp;&ensp;&ensp;训练算法伪代码如下：

![algo](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/algo.png?raw=true)

#### 目标函数改进

&ensp;&ensp;&ensp;在证明GAN能够做得拟合真实分布时，Goodfellow做了一个很大胆的假设：用来评估样本真实度的D网络具有无限的建模能力，也就是说不管真实样本和生成的样本有多复杂，D-网络都能把他们区分开。这个假设呢，也叫做非参数假设。 当然，对于深度网络来说，咱只要不断的加高加深，这还不是小菜一碟吗？深度网络擅长的就是干这个的么。

&ensp;&ensp;&ensp;但是，一旦真实样本和生成样本之间重叠可以忽略不计（这非常可能发生，特别当这两个分布是低维流型的时候），而又由于D-网络具有非常强大的无限区分能力，可以完美地分割这两个无重叠的分布，这时候，经典GAN用来优化其生成网络（下文称G-网络）的目标函数--`JS散度`就会变成一个常数！

&ensp;&ensp;&ensp;我们知道，深度学习算法，基本都是用梯度下降法来优化网络的。一旦优化目标为常数，其梯度就会消失，也就会使得无法对G-网络进行持续的更新，从而这个训练过程就停止了。这个难题一直一来都困扰这GAN的训练，称为梯度消失问题。

&ensp;&ensp;&ensp;由此对上面的目标函数进行分析，如果判别器达到最优后，由于生成器的目标函数使用的是判别器目标函数的相反数，所以，此时生成器也无法继续优化，那么问题就来了，实际情况中肯定存在判别器达到最好之后，而生成器还不是最优的情况，所以将目标函数改进如下：

![nsg](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/nsg.png?raw=true)

&ensp;&ensp;&ensp;这样：

* 不再使用一个单一的loss
* 生成器最大化判别器没有考虑到的似然概率
* 当判别器达到最优时，生成器仍然可以继续优化

&ensp;&ensp;&ensp;`KL`散度:

$$
D\_{KL}(P\Vert Q)=\int\_{-\infty}^{\infty}p(x)log\frac{p(x)}{q(x)}dx
$$

&ensp;&ensp;&ensp;`KL`散度中当`p`和`q`的位置变化后影响图：

![GAN_KL](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/GAN_KL.png?raw=true)

&ensp;&ensp;&ensp;当`p`在左边时：

$$
\begin{align}
D\_{KL}(P\Vert Q)&=\int\_{-\infty}^{\infty}p(x)log\frac{p(x)}{q(x)}dx\\\\
&=\int\ p(x)log\ p(x)-\int\ p(x)log\ q(x)\\\\
&=-常熟-常熟log\ q(x)\\\\
&\propto --log\ q(x)
\end{align}
$$

&ensp;&ensp;&ensp;最后的这个形式就是最大似然估计的形式，所以看上图左边，虽然真实分布是双峰，但是使用最大似然估计得到的分布是单峰的。同样的原理可以分析右边的图：真实分布是双峰，生成分布只拟合其中一个峰。

&ensp;&ensp;&ensp;然后就可以用`KL`散度写损失函数了：

$$
J^{(D)}=-\frac{1}{2}E\_{x~p\_{data}}log\ D(x)-\frac{1}{2}E\_zlog(1-D(G(z)))\\\\
J^{(G)}=-\frac{1}{2}E\_zexp(\sigma^{-1}(D(G(z))))
$$

&ensp;&ensp;&ensp;上面损失函数的推导可以看下面的参考链接。

&ensp;&ensp;&ensp;当判别器达到最优时，生成器的梯度达到最大似然概率。

&ensp;&ensp;&ensp;`Goodfellow`经过实验得到的三种损失函数的比较图如下：

![GAN_loss_compare](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/GAN_loss_compare.png?raw=true)

## 总结

&ensp;&ensp;&ensp;GAN是一种通过对输入的随机噪声z （比如高斯分布或者均匀分布），运用一个深度网络函数G(z)，从而希望得到一个新样本，该样本的分布，我们希望能够尽可能和真实数据的分布一致。

## 参考链接

* “On Distinguishability Criteria for Estimating Generative Models”, Goodfellow 2014
