---
title: GAN初涉
date: 2017-09-07 21:46:42
categories:
  - DL
tags:
  - GAN
---

## 正文

&ensp;&ensp;&ensp;论文原文[Generative Adversarial Nets](https://arxiv.org/abs/1406.2661)

&ensp;&ensp;&ensp;生成对抗网络是近几年研究火热的方向，为了赶上潮流，本人最近也要开始研究(咳咳，有点把科研说的俗了QAQ)。

&ensp;&ensp;&ensp;生成对抗网络是一种生成式模型，使用一个对抗过程训练网络，而不再像已有方式那样使用复杂的马尔科夫等求解。

&ensp;&ensp;&ensp;那么问题来了，什么是生成式模型呢。生成模型是相对判别模型来说的。判别模型很简单，机器学习算法的作用一般是给定样本x，得到其所在的类别(分类)、预测结果（回归）。而学习这个过程的不同分为生成模型和判别模型。

### 判别模型

&ensp;&ensp;&ensp;如果是通过学习样本x和结果y之间的条件概率P(y|x)或者y=f(x)的方式称为判别模型。事实上，y=f(x)的隐含也是学习了P(y|x)。
### 生成模型

<img src="http://chart.googleapis.com/chart?cht=tx&chl= }" style="border:none;"> 

### 


## 参考链接


熟悉经典GAN的读者都知道，GAN是一种通过对输入的随机噪声z （比如高斯分布或者均匀分布），运用一个深度网络函数G(z)，从而希望得到一个新样本，该样本的分布，我们希望能够尽可能和真实数据的分布一致（比如图像、视频等）。
在证明GAN能够做得拟合真实分布时，Goodfellow做了一个很大胆的假设：用来评估样本真实度的Discriminator网络（下文称D-网络）具有无限的建模能力，也就是说不管真实样本和生成的样本有多复杂，D-网络都能把他们区分开。这个假设呢，也叫做非参数假设。
当然，对于深度网络来说，咱只要不断的加高加深，这还不是小菜一碟吗？深度网络擅长的就是干这个的么。
但是，正如WGAN的作者所指出的，一旦真实样本和生成样本之间重叠可以忽略不计（这非常可能发生，特别当这两个分布是低维流型的时候），而又由于D-网络具有非常强大的无限区分能力，可以完美地分割这两个无重叠的分布，这时候，经典GAN用来优化其生成网络（下文称G-网络）的目标函数--JS散度-- 就会变成一个常数！
我们知道，深度学习算法，基本都是用梯度下降法来优化网络的。一旦优化目标为常数，其梯度就会消失，也就会使得无法对G-网络进行持续的更新，从而这个训练过程就停止了。这个难题一直一来都困扰这GAN的训练，称为梯度消失问题。

