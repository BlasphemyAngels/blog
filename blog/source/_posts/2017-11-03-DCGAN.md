---
title: DCGAN
copyright: true
date: 2017-11-02 16:06:42
password:
mathjax: true
categories:
  - DL
tags:
  - GAN
  - DCGAN
---

<blockquote>前面咱们介绍了`GAN`，但是`GAN`的训练是不稳定的，`DCGAN`通过将`GAN`和`CNN`结合，并在上面加入一些限制，使得`DCGAN`训练较为稳定，并且能够使用`DCGAN`训练得到好的图像特征表示，然后使用网络的生成器和判别器作为特征抽取用于有监督任务。</blockquote>

<!--more-->

### 模型

&ensp;&ensp;&ensp;文章的方法是基于最近在`CNN`架构上的三种变化提出的。

* [Springenberg et al., 2014](https://arxiv.org/abs/1412.6806)提出的全卷积网络，使用大步长的卷积操作代替`采样`操作，这样使得网络学习它自己的空间采样。文章将这种方法用在了判别器和生成器中。
* 消除卷积网络上端的全连接层。[Mordvintsev et al.](https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html)在图像分类任务中使用全局平均采样代替全连接层，作者发现使用全局平均采用能够增加模型的稳定性而降低模型的收敛速度。因此文本直接将生成器和判别器的输入和输出的全连接层移除，而都采用卷积操作（卷积在一定程度上就是采样）。
* 在除了生成器的输出层和判别器的输入层之外，每一层都使用`Batch Normalization(BN,批处理化)`。`BN`归一化每个神经元的输入为均值为0方差为1，这样解决了由低初始化引起的一些问题而且使得梯度在深度网络中传输的更好。`BN`已经被证明在`GAN`中很有用，使得`GAN`不会集中所有采样于一个点。
* 对于生成器，除了输出层使用`tanh`激励函数以为其余层均使用`ReLU`。对于判别器，所有层都使用`LeakyReLU`。
&ensp;&ensp;&ensp;模型的总体结构如下：

![DCGAN_model](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/DCGAN_model.png?raw=true)
&ensp;&ensp;&ensp;上面是生成器，下面是判别器。

&ensp;&ensp;&ensp;生成器的模型其实就是卷积的逆操作，如下图：

![DCGAN_dcnn](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/DCGAN_dcnn.png?raw=true)

### 训练

#### 训练参数
* 训练图像使用`tanh`将数值约束到`[-1,1]`
* batch size为128
* 初始化使用均值为0，方差为0.02的高斯分布
* `LeakyReLU`参数为0.2
* 使用`Adam Optimizer`，学习率设置为0.0002，动量项\\(\beta\_1\\)为0.5

#### 实验

&ensp;&ensp;&ensp;关于实验这里就不多说了，详细看原论文。

&ensp;&ensp;&ensp;下图是模型在`LSUN`上面的效果。

#### 向量算数

&ensp;&ensp;&ensp;在`词嵌入`中，词向量`国王`-词向量`男人`+词向量`女人`的结果向量是一个跟词向量`女王`很相似的向量。

&ensp;&ensp;&ensp;作者发现对于生成器向量`z`也有这个特点，如下图:

![DCGAN_va](an with glasses, then sub)

## 总结

&ensp;&ensp;&ensp;虽然`DCGAN`在原始`GAN`中加入了很多限制使得其训练变得稳定，但是实际上`DCGAN`的训练还是不太稳定。
