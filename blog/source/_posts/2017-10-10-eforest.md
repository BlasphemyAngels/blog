---
title: Auto-encoder by forest
date: 2017-10-11 19:55:51
categories:
  - DL
tags:
  - ensemble learning
  - Auto-encoder
  - forest
---
## 正文
&ensp;&ensp;&ensp;国内大神周志华即前段时间发表的媲美（号称)深度神经网络的深度森林之后又发表了用森林实现自动编码器。作为国内机器学习界首屈一指的大牛，不管其提出的模型效果怎样，阅读其文章肯定能使我们学习一些有用的东西。怀着这样的心情，我阅读了这篇文章。
[论文原文](https://arxiv.org/pdf/1709.09018.pdf)
<!--more-->
### 摘要
&ensp;&ensp;&ensp;自动编码器是一种很重要的算法，一般使用深度神经网络实现。文章提出一种全新实现自动编码其的方法：使用森林实现。文章提出一种方法使用决策树的一些信息完成编码和解码操作。实验显示，eforest有低的重建误差，而且效率高。
### 引言
&ensp;&ensp;&ensp;自动编码器是一类将输入数据映射到隐藏空间，然后再映射到原始空间的模型，它使用重建误差作为目标函数。自动编码器分为两个过程：编码和解码。编码过程将原始数据映射到隐藏空间，解码数据将数据从隐藏空间映射到原始数据空间。传统实现这两个过程的方式是使用神经网络。
&ensp;&ensp;&ensp;集成学习是一种强大的学习方法。它通过使用训练多个学习期，然后将这些学习器结合起来完成任务。
&ensp;&ensp;&ensp;决策树集成方法是集成学习中一种常用方法，bagging和boosting都通常采用决策树作为子学习器。随机森林是bagging的变种，GBDT是boosting的变种。GBDT有很多有效的实现，如XGBOOST。
&ensp;&ensp;&ensp;文章提出了一种编码森林（EncoderForest），通过一个集成树模型进行前向编码和反向解码，而且可以使用监督或者无监督训练。实验显示EncoderFroest有如下优点:
* 准确： 它的实验重建误差比使用MLP和CNN的自动编码器低。
* 有效： efroest在一个单一KNL(多核CPU)上运行比CNN-Base自动编码器在一个Titan-X GPU上运行还快。
* 容错率：训练好的模型能够正常运行即使模型部分损坏。
* 重利用： 在同一个领域下，使用一个数据集训练的模型可以直接应用到另一个数据集下。
### 模型
&ensp;&ensp;&ensp;自动编码器有连个基本的功能，编码和解码。对于一个森林来说，编码并不困难，因为至少其叶节点信息可以被看做一种编码，甚至可以说，结点集合的一个子集或者路径的分支都能够为编码提供更多的信息。
#### 编码过程
&ensp;&ensp;&ensp;首先我们提出模型的编码过程。对于给定的一个训练过的有T棵树的集成树模型（也可以是空的森林，编码过程即使森林形成过程），前向编码过程将输入数据送到每棵树的根节点，并计算每棵树，得到其所属的叶节点，最后返回一个T维向量，这个T维向量的每一项是每棵树中求到的上述叶节点在树中的编号。注意，算法跟决策树的分割规则无关。只需要是T棵树即可。
&ensp;&ensp;&ensp;算法伪代码如下：
![ae_algo1](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/ae_algo1.png?raw=true)
#### 解码过程
&ensp;&ensp;&ensp;一般来说，决策树都是用来前向预测，将数据计算从树的根节点到叶子结点，但是其反向重建是未定义的。
&ensp;&ensp;&ensp;下面通过一个小例子来探索解码过程。
&ensp;&ensp;&ensp;假设我们正在解决一个二分类问题，数据有三个属性，第一个和第二个属性是数值型属性，第三个属性是布尔型属性（取值为`YES, NO`），第四个属性是一个三值属性，取值可以是`RED，BLUE，GREEN`。给定一个实例`x`，`xi`代表`x`的第`i`个属性值。
&ensp;&ensp;&ensp;现在我们假设我们在编码过程形成一个森林，如下图：
![ae_eforest_forest](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/ae_eforest_forest.png?raw=true)
&ensp;&ensp;&ensp;现在我们只知道，实例`x`落在每棵树的哪个结点，上图中的红色结点，我们的目标是重构实例`x`。
&ensp;&ensp;&ensp;文章提出了一种简洁有效的反向重建方法。
&ensp;&ensp;&ensp;首先，在树中的每个叶子结点对应于一条唯一的从根到叶子的路径。在上面的图中已经用红色标出这样的路径。
&ensp;&ensp;&ensp;然后，每条路径都会对应一条符号规则，所以我们就得到了n条（树的数目）符号规则：
![ae_eforest_rule](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/ae_eforest_rule.png?raw=true)
&ensp;&ensp;&ensp;然后，我们可以根据上面的规则推出MCR(最大完备规则)，最大完备规则的意思是，在规则中的每一个约束的范围不能再扩大。如果扩大，则会产生冲突。
&ensp;&ensp;&ensp;例如，由上面的规则集我们可以得到MCR:
![ae_eforest_mcr](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/ae_eforest_mcr.png?raw=true)
&ensp;&ensp;&ensp;可以发现MCR中每一项都不能再扩大范围了，如果扩大会与上面的规则集中的规则的某一项产生冲突。
&ensp;&ensp;&ensp;那么显然，原始的数据肯定落在有MCR定义的范围内。
&ensp;&ensp;&ensp;计算完MCR之后，就可以根据MCR重构原始样本了，目录型属性如上面的第三和第四属性只需要根据MCR中的指定取即可，而数值型属性则可以根据MCR中的范围取一个特殊值即可（中位数、均值、或者最大最小值）。
&ensp;&ensp;&ensp;计算MCR的伪代码如下：
![ae_eforest_algo2](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/ae_eforest_algo2.png?raw=true)
&ensp;&ensp;&ensp;根据上面的描述，现在可以总结一下解码的过程。给定T棵树和编码完的T维向量:
&ensp;&ensp;&ensp;首先根据编码完的T维向量从树中得到T个决策规则，再根据这些规则得到MCR，再根据MCR重构得到x，算法如下：
![ae_eforest_algo3](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/ae_eforest_algo3.png?raw=true)
### 实验
![ae_eforest_exp_image](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/ae_eforest_exp_image.png?raw=true)
![ae_eforest_exp_image2](https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/ae_eforest_exp_image2.png?raw=true)
